---
profile::kafka::zookeeper_nodes: "%{::zookeeper_nodes}"
profile::kafka::storage_device: "%{::storage_device}"
profile::kafka::kafka_cluster_id: "%{::kafka_cluster_id}"
profile::kafka::kafka_yaml_profile_name: "%{::kafka_yaml_profile_name}"
monitoring::jmx_exporter::jmx_exporter_service: 'kafka'
kafka::broker::opts: "-javaagent:/opt/jmx_exporter-%{hiera('monitoring::jmx_exporter::version')}/jmx_prometheus_javaagent-%{hiera('monitoring::jmx_exporter::version')}.jar=%{hiera('jmx_exporter_port')}:/opt/jmx_exporter-%{hiera('monitoring::jmx_exporter::version')}/kafka.yaml"


profile::common::cloudwatchlogs::recursive: false
cloudwatchlog_files:
  "/talend/tic/%{::main_stack}/%{::sub_stack}/%{::puppet_role}/var/log/cfn-init.log":
    path: '/var/log/cfn-init.log'
  "/talend/tic/%{::main_stack}/%{::sub_stack}/%{::puppet_role}/var/log/cfn-init-cmd.log":
    path: '/var/log/cfn-init-cmd.log'
  "/talend/tic/%{::main_stack}/%{::sub_stack}/%{::puppet_role}/var/log/messages":
    path: '/var/log/messages'
  "/talend/tic/%{::main_stack}/%{::sub_stack}/%{::puppet_role}/var/log/secure":
    path: '/var/log/secure'
  "/talend/tic/%{::main_stack}/%{::sub_stack}/%{::puppet_role}/var/log/audit/audit.log":
    path: '/var/log/audit/audit.log'
    datetime_format: '%s'
  "/talend/tic/%{::main_stack}/%{::sub_stack}/%{::puppet_role}/opt/kafka/logs/server.log":
    path: '/opt/kafka/logs/server.log'
  "/talend/tic/%{::main_stack}/%{::sub_stack}/%{::puppet_role}/opt/kafka/logs/state-change.log":
    path: '/opt/kafka/logs/state-change.log'
  "/talend/tic/%{::main_stack}/%{::sub_stack}/%{::puppet_role}/opt/kafka/logs/kafka-request.log":
    path: '/opt/kafka/logs/kafka-request.log'
  "/talend/tic/%{::main_stack}/%{::sub_stack}/%{::puppet_role}/opt/kafka/logs/log-cleaner.log":
    path: '/opt/kafka/logs/log-cleaner.log'
  "/talend/tic/%{::main_stack}/%{::sub_stack}/%{::puppet_role}/opt/kafka/logs/controller.log":
    path: '/opt/kafka/logs/controller.log'
  "/talend/tic/%{::main_stack}/%{::sub_stack}/%{::puppet_role}/opt/kafka/logs/kafka-authorizer.log":
    path: '/opt/kafka/logs/kafka-authorizer.log'

# Default profile
profile::kafka::kafka_broker_config:
  log.cleanup.policy: 'delete'
  log.retention.bytes: "268435456" # 256 MiB, per partition
  log.segment.bytes: "134217728"   # 128 MiB max per segment
  log.roll.ms: "1200000"           # a segment will be used (written) for max 20mn (after 20mn, a new file is created)
  log.retention.ms: "43200000"     # 12h

# We can have several kafka cluster
kafka_infra_logs_cluster_ha: # estimated max cluster size: 102 GiB + customers offsets => 60GiB per node
  kafka_topics_default_partitions: 12
  kafka_topics_default_replication: 2
  log_level: 'INFO'
  kafka_broker_config:
    log.cleanup.policy: 'delete'
    log.retention.bytes: "268435456" # 256 MiB, per partition
    log.segment.bytes: "134217728"   # 128 MiB max per segment
    log.roll.ms: "1200000"           # a segment will be used (written) for max 20mn (after 20mn, a new file is created)
    log.retention.ms: "43200000"     # 12h
  topics:
    tpsvclogs:
      topic_options:
        retention.ms: "43200000"      # 12h
        retention.bytes: "2147483648" # 2 GiB per partition
    systemlogs:
      topic_options:
        retention.ms: "43200000"      # 12h
        retention.bytes: "2147483648" # 2 GiB per partition
    zipkin: {}

kafka_customers_logs_cluster_ha: # estimated max cluster size: 308 GiB + customers offsets => 120GiB per node
  kafka_topics_default_partitions: 12
  kafka_topics_default_replication: 2
  log_level: 'INFO'
  kafka_broker_config:
    log.cleanup.policy: 'delete'
    log.retention.bytes: "268435456" # 256 MiB, per partition
    log.segment.bytes: "134217728"   # 128 MiB max per segment
    log.roll.ms: "1200000"           # a segment will be used (written) for max 20mn (after 20mn, a new file is created)
    log.retention.ms: "43200000"     # 12h
  topics:
    tpsvclogs:
      topic_options:
        retention.ms: "43200000"      # 12h
        retention.bytes: "4294967296" # 4GiB per partition
    events-to-platform:
      topic_options:
        retention.ms: "43200000"      # 12h
        retention.bytes: "536870912" # 512MiB per partition
    tmc_logs:
      partitions: "30"
      topic_options:
        retention.ms: "43200000"        # 12h
        retention.bytes: "3586297696"   # 3.34GiB per partition
        max.message.bytes: "104857600"  # 100 MiB, align with ActiveMQ max message size
        segment.ms: "300000"            # 5mn

kafka_applications_cluster_ha: # estimated max cluster size: 102 GiB (24 topics with default -3 GiB-  + 4 with custom size) + customers offsets  => 55 GiB per node with security
  kafka_topics_default_partitions: 12
  kafka_topics_default_replication: 2
  log_level: 'INFO'
  kafka_broker_config:
    log.cleanup.policy: 'delete'
    log.retention.bytes: "134217728" # 128 MiB, per partition => default topic sizing: 12 * 128MiB * 2 = 3 GiB per topic on cluster
    log.segment.bytes: "67108864"    # 64 MiB max per segment
    log.roll.ms: "1200000"           # a segment will be used (written) for max 20mn (after 20mn, a new file is created)
    log.retention.ms: "43200000"     # 12h
  topics:
    dispatcher: {}
    container-manager: {}
    container-events: {}
    output: {}
    data-history: {}
    schemas: {}
    schemas-references: {}
    dataset-changed:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    datastore-changed:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    app-to-runtime:
      topic_options:
        retention.ms: "3600000"       # 1H
        retention.bytes: "268435456"  # 256 MiB => 6 GiB per topic
        max.message.bytes: "10485760" # 10 MiB, specific consumer conf needed
        segment.ms: "300000"          # 5mn
    runtime-to-app:
      topic_options:
        retention.ms: "3600000"       # 1H
        retention.bytes: "268435456"  # 256 MiB => 6 GiB per topic
        max.message.bytes: "10485760" # 10 MiB, specific consumer conf needed
        segment.ms: "300000"          # 5mn
    notifications: {}
    websocket-to-app:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    app-to-websocket:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    logs-runtime-to-app:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    userflow-changed:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    impact-analysis-batch: {}
    dataprep:
      topic_options:
        retention.ms: "1800000" # 30min
        segment.ms: "300000"    # 5mn
    dataprep-unique:
      topic_options:
        retention.ms: "1800000" # 30min
        segment.ms: "300000"    # 5mn
    dataprep-broadcast:
      topic_options:
        retention.ms: "1800000" # 30min
        segment.ms: "300000"    # 5mn
    provisioning: {}
    tpsvc-salesforce-contact:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    rating-changed:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    favorites-changed:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    sharing-changed:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    tds-task-notification:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    tpsvc-engines-events:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    tsd-semantic-update:
      topic_options:
        retention.bytes: "536870912" # 512 MiB per partition => 12GiB per cluster
        retention.ms: "648000000"    # 7.5d
        segment.ms: "3600000"        # 1h
    tsd-semantic-notification:
      topic_options:
        retention.bytes: "268435456" # 256 MiB per partition => 6GiB per cluster
        retention.ms: "86400000"    # 1d
        segment.ms: "300000"        # 5mn
    tmc-live-updates:
      topic_options:
        retention.ms: "900000"   # 15mn
        segment.ms: "300000"    # 5mn

kafka_applications_cluster_simple:   # estimated max data size: 30,5 GiB (24 topics with default + 4 with custom size) + customer_offsets
  kafka_topics_default_partitions: 2
  kafka_topics_default_replication: 1
  kafka_broker_config:
    log.cleanup.policy: 'delete'
    log.retention.bytes: "536870912" # 512MiB, per partition => default topic sizing: 2 * 528MiB = 1 GiB per topic on cluster
    log.segment.bytes: "67108864"    # 64 MiB max per segment
    log.roll.ms: "1200000"           # a segment will be used (written) for max 20mn (after 20mn, a new file is created)
    log.retention.ms: "43200000"     # 12h
  topics:
    dispatcher: {}
    container-manager: {}
    container-events: {}
    output: {}
    data-history: {}
    schemas: {}
    schemas-references: {}
    dataset-changed:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    datastore-changed:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    app-to-runtime:
      topic_options:
        retention.ms: "3600000" # 1H
        retention.bytes: "268435456"  # 256 MiB => 512 MiB per topic
        max.message.bytes: "10485760" # 10 MiB, specific consumer conf needed
        segment.ms: "300000"    # 5mn
    runtime-to-app:
      topic_options:
        retention.ms: "3600000" # 1H
        retention.bytes: "268435456"  # 256 MiB => 512 MiB per topic
        max.message.bytes: "10485760" # 10 MiB, specific consumer conf needed
        segment.ms: "300000"    # 5mn
    notifications: {}
    websocket-to-app:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    app-to-websocket:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    logs-runtime-to-app:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    userflow-changed:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    impact-analysis-batch: {}
    dataprep:
      topic_options:
        retention.ms: "1800000" # 30min
        segment.ms: "300000"    # 5mn
    dataprep-unique:
      topic_options:
        retention.ms: "1800000" # 30min
        segment.ms: "300000"    # 5mn
    dataprep-broadcast:
      topic_options:
        retention.ms: "1800000" # 30min
        segment.ms: "300000"    # 5mn
    provisioning: {}
    tpsvc-salesforce-contact:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    rating-changed:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    favorites-changed:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    sharing-changed:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    tds-task-notification:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    tpsvc-engines-events:
      topic_options:
        retention.ms: "3600000" # 1H
        segment.ms: "300000"    # 5mn
    tsd-semantic-update:
      topic_options:
        retention.bytes: "2684354560" # 2,5GiB => 5GiB per topic
        retention.ms: "648000000" # 7.5d
        segment.ms: "3600000"     # 1h
    tsd-semantic-notification:
      topic_options:
        retention.bytes: "268435456" # 256 MiB per partition => 512 MiB per cluster
        retention.ms: "86400000"    # 1d
        segment.ms: "300000"        # 5mn
    tmc-live-updates:
      topic_options:
        retention.ms: "900000"   # 15mn
        segment.ms: "300000"    # 5mn

# Example of tuning for profiling kafka:
# <kafka_yaml_profile_name>:
#   kafka_version: '0.10.2.1' # if absent, use profile::kafka::kafka_version
#   scala_version: '2.11'     # if absent, use profile::kafka::scala_version
#   log_level: 'INFO'         # if absent, use profile::kafka::log_level
#   kafka_topics_default_replication: 1 # if absent, use profile::kafka::kafka_topics_default_replication
#   kafka_topics_default_partitions: 6  # if absent, use profile::kafka::kafka_topics_default_partitions
#   kafka_broker_config:  # a merge is done between this hash, profile::kafka::kafka_broker_config and the default
#     log.cleanup.policy: 'delete'
#     log.retention.bytes: "536870912"
#     log.retention.ms: "43200000"
#   topics:  # a merge is done between this hash and profile::kafka::kafka_topics_config
#     <topic1name>:
#       replication_factor: 1
#       partitions: 1
#       topic_options: # https://kafka.apache.org/documentation/#topic-config
#         retention.bytes: "1073741824"
#     <topic2name_all_default>: {}
#
